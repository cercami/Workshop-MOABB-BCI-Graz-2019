{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical BCI Pipeline\n",
    "\n",
    "In this notebook, we will go through all the steps to make a simple BCI classification task, downloading a dataset and using a standard classifier. We choose the dataset 2a from BCI Competition IV, a motor imagery task. We will use a CSP to enhance the signal-to-noise ratio of the EEG epochs and a LDA to classify these signals.\n",
    "\n",
    "First, we need to import all the methods for reading the data, filtering and pre-processing the EEG, classifying the signals and plotting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne import create_info\n",
    "from mne.io import RawArray\n",
    "from mne.channels import read_montage\n",
    "import glob\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mne.decoding import CSP\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking up a dataset\n",
    "\n",
    "We will work on the *BCI Competition IV Dataset 2a*. This dataset consists of EEG data from 9 subjects. The cue-based BCI paradigm consisted of four different motor imagery tasks, we will only focus on the left hand (class 1) and right hand (class 2). Two sessions on different days were recorded for each subject. Each session is comprised of 6 runs separated by short breaks. One run consists of 48 trials (12 for each of the four possible classes), yielding a total of 288 trials per session. Considering only left and right trials, we have 144 trials by session.\n",
    "\n",
    "The cue is shown to the subjects two seconds ($t$ = 2s) after the start of the trial and the subjects were ask to carry out the motor imagery task until the fixation cross disappeared from the screen at $t$ = 6s.\n",
    "\n",
    "Twenty-two Ag/AgCl electrodes were used to record the EEG. All signals were recorded monopolarly with the left mastoid serving as reference and the right mastoid as ground. The signals were sampled at 250 Hz and bandpass-filtered between 0.5 Hz and 100 Hz. The sensitivity of the amplifier was set to 100 $\\mu$V. An additional 50 Hz notch filter was enabled to suppress line noise\n",
    "\n",
    "We will need to download the data either manually from the [BNCI website](http://bnci-horizon-2020.eu/database/data-sets) or with the following commands that should work on Linux or Mac:\n",
    "\n",
    "`mkdir -p ../dataset/001-2014`\n",
    "\n",
    "`for f in A01T A01E A02T A02E A03T A03E A04T A04E A05T A05E A06T A06E A07T A07E A08T A08E A09T A09E; do curl -C - -o ../dataset/001-2014/\"$f\".mat https://lampx.tugraz.at/~bci/database/001-2014/\"$f\".mat; done`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One session at a time\n",
    "\n",
    "Let's focus on the first subject, picking one session with 6 runs of 48 trials, that is 144 trials for left and right hand imagery. For the sake of the example, we will exclude feet and tongue trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../dataset/001-2014/A01E.mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data\n",
    "\n",
    "We need to load the file `A01E.mat` that is store as MATLAB file with scipy loadmat function. From the .mat file, we need to get the actual EEG data that are stored as a list of runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "event_id = {}\n",
    "data = loadmat(filename, struct_as_record=False, squeeze_me=True)\n",
    "run_array = data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each run, we need to identify the correct electrodes names and position and there is 3 EOG channels. We use MNE to infer electrode position from their name (using 10-20 system). To benefit from MNE, we need to provide some information: channel names and types, sampling frequency and montage. We also need to keep track events id. \n",
    "For further processing, we need to supply the sampling frequency `sfreq` to MNE.\n",
    "\n",
    "Instead of just 6 runs, there is 9 runs because there are 3 baseline runs that do not contains any trials, only resting state. We need to discard those runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in run_array:\n",
    "\n",
    "    ch_names = [\n",
    "        'Fz', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'C5', 'C3', 'C1', 'Cz', 'C2',\n",
    "        'C4', 'C6', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'P1', 'Pz', 'P2', 'POz',\n",
    "        'EOG1', 'EOG2', 'EOG3'\n",
    "    ]\n",
    "    ch_types = ['eeg'] * 22 + ['eog'] * 3    \n",
    "    \n",
    "    evd = {}\n",
    "    n_chan = run.X.shape[1]\n",
    "    montage = read_montage('standard_1005')\n",
    "    eeg_data = 1e-6 * run.X\n",
    "    sfreq = run.fs\n",
    "\n",
    "    trigger = np.zeros((len(eeg_data), 1))\n",
    "    # some runs does not contains trials i.e baseline runs\n",
    "    if len(run.trial) > 0:\n",
    "        trigger[run.trial - 1, 0] = run.y\n",
    "\n",
    "    eeg_data = np.c_[eeg_data, trigger]\n",
    "    ch_names = ch_names + ['stim']\n",
    "    ch_types = ch_types + ['stim']\n",
    "    evd = {ev: (ii + 1) for ii, ev in enumerate(run.classes)}\n",
    "    info = create_info(ch_names=ch_names, ch_types=ch_types, sfreq=sfreq, montage=montage)\n",
    "    rawi = RawArray(data=eeg_data.T, info=info, verbose=False)    \n",
    "    \n",
    "    runs.append(rawi)\n",
    "    event_id.update(evd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and filter \n",
    "\n",
    "We could now concatenate all trial runs to make a session. It is also a good time to filter the EEG between 8 and 32 Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RawArray  |  None, n_channels x n_times : 26 x 686200 (2744.8 sec), ~136.2 MB, data loaded>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = mne.concatenate_raws(runs)\n",
    "raw.filter(l_freq=8, h_freq=32, method='iir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract the trials across all the runs; we pick the signal between $t+2$ to $t+6$s, gathering trials only from left and right hand class, excluding other classes. Relying on MNE, it is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = mne.find_events(raw)\n",
    "epochs = mne.Epochs(raw, events, event_id={'left hand':1, 'right hand':2}, tmin=2, tmax=6, \n",
    "                    baseline=None, picks=mne.pick_types(raw.info, eeg=True, stim=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "\n",
    "The EEG will now be processed by scikit-learn, we need to extract the data $X$ and the label $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = epochs.get_data()\n",
    "y = epochs.events[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Scikit pipeline object allows to automate preprocessing and classification task. It also eases the parameter search or optimization as we will see later.\n",
    "\n",
    "Our pipeline is simple, we want to apply a [CSP](https://my.ece.utah.edu/~ece6534/notes/2017_ece6534_lecture28.pdf) to filter the data using the eigenvectors associated with the four largest and four smallest eigenvalues. The resulting signal will be send to a simple [Linear Disciminant Analysis](https://scikit-learn.org/stable/modules/lda_qda.html#lda-qda).\n",
    "\n",
    "Scikit-learn expects labels to be between 0 and `n_classes-1`. To avoid potential problems if the trials are not labeled accordingly, we use [LabelEncoder](https://scikit-learn.org/stable/modules/preprocessing_targets.html#preprocessing-targets) to ensure that all labels follows scikit-learn convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(CSP(n_components=8), LDA())\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With scikit-learn, all the evaluation process is simplified. Here, we want to make a [$k$-fold validation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) with $k=5$. Using [AUC](https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics), our evaluation score is the mean AUC on the $k$-fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score is 94.6\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(5, shuffle=False)\n",
    "acc = cross_val_score(pipeline, X, y, cv=cv, scoring='roc_auc')\n",
    "print('AUC score is {:.1f}'.format(acc.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain an AUC score of 94.6 which is similar to common results obtained for subject 1 of this dataset.\n",
    "\n",
    "## Extending to all subjects\n",
    "\n",
    "To process the the data from all the subjects, we will gather all the code from above and put it in a function `get_session_score`. Given a filename and scikit pipeline, this function returns the mean AUC.\n",
    "\n",
    "To keep the notebook clean, we discarded all the printed logs (warnings and information) but this is because we have already tested the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "mne.set_log_level(\"CRITICAL\")\n",
    "\n",
    "def get_session_score(filename, pipeline):\n",
    "    \n",
    "    runs = []\n",
    "    event_id = {}\n",
    "    data = loadmat(filename, struct_as_record=False, squeeze_me=True)\n",
    "    run_array = data['data']\n",
    "    for run in run_array:\n",
    "\n",
    "        ch_names = [\n",
    "            'Fz', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'C5', 'C3', 'C1', 'Cz', 'C2',\n",
    "            'C4', 'C6', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'P1', 'Pz', 'P2', 'POz',\n",
    "            'EOG1', 'EOG2', 'EOG3'\n",
    "        ]\n",
    "        ch_types = ['eeg'] * 22 + ['eog'] * 3    \n",
    "\n",
    "        evd = {}\n",
    "        n_chan = run.X.shape[1]\n",
    "        montage = read_montage('standard_1005')\n",
    "        eeg_data = 1e-6 * run.X\n",
    "        sfreq = run.fs\n",
    "\n",
    "        if not ch_names:\n",
    "            ch_names = ['EEG%d' % ch for ch in range(1, n_chan + 1)]\n",
    "            montage = None  # no montage\n",
    "\n",
    "        if not ch_types:\n",
    "            ch_types = ['eeg'] * n_chan\n",
    "\n",
    "        trigger = np.zeros((len(eeg_data), 1))\n",
    "        # some runs does not contains trials i.e baseline runs\n",
    "        if len(run.trial) > 0:\n",
    "            trigger[run.trial - 1, 0] = run.y\n",
    "\n",
    "        eeg_data = np.c_[eeg_data, trigger]\n",
    "        ch_names = ch_names + ['stim']\n",
    "        ch_types = ch_types + ['stim']\n",
    "        evd = {ev: (ii + 1) for ii, ev in enumerate(run.classes)}\n",
    "        info = create_info(ch_names=ch_names, ch_types=ch_types, sfreq=sfreq, montage=montage)\n",
    "        rawi = RawArray(data=eeg_data.T, info=info, verbose=False)    \n",
    "\n",
    "        runs.append(rawi)\n",
    "        event_id.update(evd)    \n",
    "        \n",
    "    raw = mne.concatenate_raws(runs)\n",
    "    raw.filter(l_freq=8, h_freq=32, method='iir')   \n",
    "    \n",
    "    events = mne.find_events(raw)\n",
    "    epochs = mne.Epochs(raw, events, event_id={'left hand':1, 'right hand':2}, tmin=2, tmax=6, baseline=None, picks=mne.pick_types(raw.info, eeg=True, stim=False))\n",
    "    X = epochs.get_data()\n",
    "    y = epochs.events[:,-1]    \n",
    "    \n",
    "    cv = StratifiedKFold(5, shuffle=False)\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    acc = cross_val_score(pipeline, X, y, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    return acc.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that the function works as expected, we apply it on our data from first subject `A01E.mat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.6\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(CSP(n_components=8), LDA())\n",
    "score = get_session_score(filename, pipeline)\n",
    "print(\"{:.1f}\".format(score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is the same, we could move to processing all session files from all 9 subjects. \n",
    "\n",
    "### Evaluation\n",
    "\n",
    "To ease the plot of the results we will use [seaborn](https://seaborn.pydata.org/) and we need to format the data in a [panda's DataFrame](https://pandas.pydata.org/pandas-docs/stable/getting_started/dsintro.html). For this notebook, we could see DataFrame as powerful dictionary-like structure. \n",
    "\n",
    "For all files from the dataset, we get the AUC score and we log the session and subject to put everything in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../dataset/001-2014/A01E.mat\n",
      "../dataset/001-2014/A01T.mat\n",
      "../dataset/001-2014/A02E.mat\n",
      "../dataset/001-2014/A02T.mat\n",
      "../dataset/001-2014/A03E.mat\n",
      "../dataset/001-2014/A03T.mat\n",
      "../dataset/001-2014/A04E.mat\n",
      "../dataset/001-2014/A04T.mat\n",
      "../dataset/001-2014/A05E.mat\n",
      "../dataset/001-2014/A05T.mat\n",
      "../dataset/001-2014/A06E.mat\n",
      "../dataset/001-2014/A06T.mat\n",
      "../dataset/001-2014/A07E.mat\n",
      "../dataset/001-2014/A07T.mat\n",
      "../dataset/001-2014/A08E.mat\n",
      "../dataset/001-2014/A08T.mat\n",
      "../dataset/001-2014/A09E.mat\n",
      "../dataset/001-2014/A09T.mat\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/results_part1-1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-9cdab108fa19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubjects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'session'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../results/results_part1-1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1743\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1745\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[1;32m    135\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                                      compression=None)\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;31m# Python 3 and encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# Python 3 and no explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../results/results_part1-1.csv'"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "subjects = []\n",
    "sessions = []\n",
    "pipeline = make_pipeline(CSP(n_components=8), LDA())\n",
    "\n",
    "for filename in glob.glob('../dataset/001-2014/*'):\n",
    "    print(filename)\n",
    "\n",
    "    score = get_session_score(filename, pipeline)    \n",
    "    scores.append(score)\n",
    "    \n",
    "    session_name = 'session_' + filename.split('/')[-1].split('.')[0][-1]\n",
    "    sessions.append(session_name)\n",
    "    \n",
    "    subject = int(filename.split('/')[-1].split('.')[0][-2])    \n",
    "    subjects.append(subject)\n",
    "    \n",
    "results = pd.DataFrame()\n",
    "results['score'] = scores\n",
    "results['subject'] = subjects\n",
    "results['session'] = sessions\n",
    "results.to_csv('../results/results_part1-1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results\n",
    "\n",
    "We could plot the results with seaborn. For each session of each subject, we have a average AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('../results/results_part1-1.csv')\n",
    "fig, ax = plt.subplots(figsize=(8,7))\n",
    "results[\"subj\"] = results[\"subject\"].apply(str)\n",
    "sns.barplot(x=\"score\", y=\"subj\", hue='session', data=results, orient='h', palette='viridis', ax=ax)\n",
    "# sns.catplot(kind='bar', x=\"score\", y=\"subj\", hue='session', data=results, orient='h', palette='viridis')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
